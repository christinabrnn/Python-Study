{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christinabrnn/Python-Study/blob/main/BA820/text_analysis_basics_unsolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Course: BA820 - Unsupervised and Unstructured ML**\n",
        "\n",
        "**Notebook created by: Mohannad Elhamod**"
      ],
      "metadata": {
        "id": "jtWlvRw2YUxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Prerequisite: Text Cleaning and Regex"
      ],
      "metadata": {
        "id": "dXf-GfaJ8eeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start with some interesting sentences. Each sentence in this context can be considered *a document*."
      ],
      "metadata": {
        "id": "wWeRoCDRCjPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"In 1945, the US dropped two nuclear bombs on Japan. Japan surrendered afterwards.\",\n",
        "    \"Japan is located in Asia. Tokyo is its capital.\",\n",
        "    \"The capital of the USA is Washington D.C., which is located on the eastern seaboard.\",\n",
        "    \"I like eating apples! I eat 2.3 pounds everyday.\",\n",
        "    \"The capitol of Canada is Ottawa. My aunt's number there is (613)-554-2121. I enjoy visiting here.\",\n",
        "    \"       5/2 = 2.5.\",\n",
        "    \"The professor was very kind to us when creating the midterm exam.\",\n",
        "    \"An apple a day keeps the doctor away!\",\n",
        "    \"I love Apple products\",\n",
        "    \"@jason We won the game! #WeAreTheChampions.\",\n",
        "    \"My phone number in Canada is (613)-224-2311        \",\n",
        "    \"Eat this apple.\"\n",
        "]\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'text':corpus})\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "qHCi0tIbJWGa",
        "outputId": "6e3e3168-2ce8-4232-9e5d-4c3ccecc950e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text\n",
              "0   In 1945, the US dropped two nuclear bombs on J...\n",
              "1     Japan is located in Asia. Tokyo is its capital.\n",
              "2   The capital of the USA is Washington D.C., whi...\n",
              "3    I like eating apples! I eat 2.3 pounds everyday.\n",
              "4   The capitol of Canada is Ottawa. My aunt's num...\n",
              "5                                          5/2 = 2.5.\n",
              "6   The professor was very kind to us when creatin...\n",
              "7               An apple a day keeps the doctor away!\n",
              "8                               I love Apple products\n",
              "9         @jason We won the game! #WeAreTheChampions.\n",
              "10  My phone number in Canada is (613)-224-2311   ...\n",
              "11                                    Eat this apple."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12fa8408-623c-40b0-a3da-31e111718440\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1945, the US dropped two nuclear bombs on J...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan is located in Asia. Tokyo is its capital.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The capital of the USA is Washington D.C., whi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I like eating apples! I eat 2.3 pounds everyday.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The capitol of Canada is Ottawa. My aunt's num...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5/2 = 2.5.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The professor was very kind to us when creatin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>An apple a day keeps the doctor away!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I love Apple products</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@jason We won the game! #WeAreTheChampions.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>My phone number in Canada is (613)-224-2311   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Eat this apple.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12fa8408-623c-40b0-a3da-31e111718440')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-12fa8408-623c-40b0-a3da-31e111718440 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-12fa8408-623c-40b0-a3da-31e111718440');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-236423d7-bb70-4450-bb95-77d864ec5a61\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-236423d7-bb70-4450-bb95-77d864ec5a61')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-236423d7-bb70-4450-bb95-77d864ec5a61 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6020cdf6-2249-4e37-8c8e-acda791e4e01\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6020cdf6-2249-4e37-8c8e-acda791e4e01 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"My phone number in Canada is (613)-224-2311        \",\n          \"@jason We won the game! #WeAreTheChampions.\",\n          \"In 1945, the US dropped two nuclear bombs on Japan. Japan surrendered afterwards.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some pre-processing you might want to consider:\n",
        "\n",
        "- lower/upper casing. *Is the effect positive or negative?*\n",
        "- Removing trailing spaces.\n",
        "- Removing punctuation. *Is the effect positive or negative?*\n",
        "- Replacing synonyms."
      ],
      "metadata": {
        "id": "PoKB7NaRZQMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_modified = df.copy()\n",
        "df_modified = pd.DataFrame(df_modified.text.str.lower()) # to lower case.\n",
        "# df_modified = pd.DataFrame(df_modified.text.str.strip()) # removing leading and trailing spaces\n",
        "\n",
        "# df_modified = pd.DataFrame(df_modified.text.str.replace(\"like\", \"enjoy\")) # synonym replacement\n",
        "# df_modified = pd.DataFrame(df_modified.text.str.replace('[^\\w\\s]','', regex=True)) # remove punctuation\n",
        "\n",
        "### returning matches\n",
        "# df_modified = pd.DataFrame(df_modified.text.str.findall('@\\S+|#\\S+'))  # list of matches\n",
        "# df_modified = df_modified[df_modified.text.str.contains(r'@\\S+|#\\S+', regex=True)] # rows that contain matches\n",
        "# df_modified = df_modified[df_modified.text.str.contains(\"us\")] # True or False\n",
        "\n",
        "df_modified"
      ],
      "metadata": {
        "id": "6X0Eh848ZQSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization: Text to Tokens\n",
        "\n",
        "In order to represent text, we need to first break it down into units."
      ],
      "metadata": {
        "id": "W04fTkA5KeXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjeOWLlf2ngu",
        "outputId": "90fafd3d-5723-4a91-da50-c037d124c7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "h3-zIteXyV5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, RegexpTokenizer\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "import nltk\n",
        "\n",
        "\n",
        "tokenized = [word_tokenize(t) for t in corpus] # word tokenization\n",
        "# tokenized = [WhitespaceTokenizer().tokenize(t) for t in corpus] # based on white spaces.\n",
        "# tokenized = [TweetTokenizer().tokenize(t) for t in corpus] #Tweets tokenization\n",
        "# tokenized = [RegexpTokenizer(r'\\d{4}|\\d{3}', gaps=False).tokenize(t) for t in corpus] # '\\([0-9]{3}\\)-[0-9]{3}-[0-9]{4}' #'\\d{4}|\\d{3}' # Regex tokenization. This keeps phone numbers only\n",
        "tokenized"
      ],
      "metadata": {
        "id": "gSsuYuMcKekQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may want to remove stop words."
      ],
      "metadata": {
        "id": "qcKp3qEb0jv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Y7kbOvsp0ple"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # Get the set of stop words\n",
        "\n",
        "tokenized_filtered =\n",
        "\n",
        "\n",
        "tokenized_filtered"
      ],
      "metadata": {
        "id": "P_tDxFPC0j1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization does not have to be at the word level... What matters is that we split the text into units in some way."
      ],
      "metadata": {
        "id": "nACwl08H-eZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import SyllableTokenizer\n",
        "\n",
        "ST = SyllableTokenizer() # Tokenizes words not sentences.\n",
        "\n",
        "tokenized_filtered_syllables = []\n",
        "for sentence in df_modified[\"text\"]:\n",
        "  tokenized_filtered_syllables.append(ST.tokenize(sentence))\n",
        "\n",
        "tokenized_filtered_syllables"
      ],
      "metadata": {
        "id": "C64iIjmI-dl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "- any interesting sentences you want to try?\n",
        "- What if I want to only collect phone numbers?\n",
        "- What if I want to only collect email addresses?\n",
        "- Is it better to tokenize by word, sentence, character, or \"sub-words?\".\n"
      ],
      "metadata": {
        "id": "YToUo0nN8wVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stemming\n",
        "\n",
        "Alternatively, we can stem words and hopefully keep the essence of the meaning each sentence. This might make text comparison easier."
      ],
      "metadata": {
        "id": "4p8QaH5p2vbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "tokenized_filtered_stemmed =\n",
        "\n",
        "tokenized_filtered_stemmed"
      ],
      "metadata": {
        "id": "W1k100tN3D5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can always put the tokens back together into one string."
      ],
      "metadata": {
        "id": "wky_U5SW5_xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "for tokenized_sentence in tokenized_filtered_stemmed:\n",
        "  print(TreebankWordDetokenizer().detokenize(tokenized_sentence))"
      ],
      "metadata": {
        "id": "msyTTWnY6Ei8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Frequency-based Vectorization"
      ],
      "metadata": {
        "id": "KttSbUqeckDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BoW"
      ],
      "metadata": {
        "id": "sE9jwrzJOPs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have tokenized the sentences, we can vectorize them. Let's use **Bag of Words**, the simplest way we know.\n",
        "\n",
        "Let's create and fit the model."
      ],
      "metadata": {
        "id": "rhLSwvahDDjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "8F7gRbqsDDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_lemmatize(sentence):\n",
        "  tokens = word_tokenize(sentence) # tokenize\n",
        "  lemmatized_tokens = [ps.stem(word) for word in tokens] # lemmatize\n",
        "  return lemmatized_tokens\n",
        "\n",
        "#model\n",
        "cv = #tokenizer= word_tokenize, tokenizer=tokenize_lemmatize stop_words='english'\n",
        "\n",
        "# fit\n",
        "cv.\n",
        "\n",
        "print('number of `tokens`', len(cv.vocabulary_))\n",
        "cv.vocabulary_"
      ],
      "metadata": {
        "id": "fI6voyQBehfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can print the list of stop words that was used"
      ],
      "metadata": {
        "id": "rkewL04C_53b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_stop_words()"
      ],
      "metadata": {
        "id": "KlvAxqjSiXLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's transform the documents into BoW format"
      ],
      "metadata": {
        "id": "ZIxbpHsFABhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtm = cv.\n",
        "bow = pd.DataFrame(, columns=)\n",
        "bow"
      ],
      "metadata": {
        "id": "u5ZMb7-HewxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "- What does this table remind you of? Is there any relevance?\n",
        "- How does the representation using TF-IDF look like?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YBTuurH8_Wh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see which tokens were extracted for a sentence using `cv.inverse_transform`"
      ],
      "metadata": {
        "id": "cMmLbUVxALW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recognized_tokens_sentence0 = cv.\n",
        "recognized_tokens_sentence0"
      ],
      "metadata": {
        "id": "nnbLhhfJlyDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "- Why did it not return the sentence back?"
      ],
      "metadata": {
        "id": "63H2lPHW_3Y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Document Similarity"
      ],
      "metadata": {
        "id": "04X1vSF-AvBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare cosine similarity vs. Euclidean distance. We will calculate the *similariy matrix*."
      ],
      "metadata": {
        "id": "RbfC8K_zFih5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, cosine similarity"
      ],
      "metadata": {
        "id": "XavZgdCEBAAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Cosine sim\n",
        "cos_sim = pd.DataFrame(  )\n",
        "cos_sim"
      ],
      "metadata": {
        "id": "wQB0-WG5FioS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try to answer a question"
      ],
      "metadata": {
        "id": "kzR1m2J4_Mc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"What' is my aunt's number'?\"\n",
        "\n",
        "q_vector = cv.transform(   ) # Get the question's represntation in terms of BOW\n",
        "\n",
        "pd.DataFrame(   ) # How similar is each sentence to that questions?"
      ],
      "metadata": {
        "id": "VsqbH8slnwwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the top two  matches are the ones with phone numbers (mine and my aunt's)."
      ],
      "metadata": {
        "id": "haDX3bCW_dim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Euclidean *similarity*"
      ],
      "metadata": {
        "id": "N8xmUQlmBIWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uclidean_distances =\n",
        "\n",
        "uclidean_similarity"
      ],
      "metadata": {
        "id": "2uTvvHytkeM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_matrix =\n",
        "sim_matrix =\n",
        "\n",
        "sim_matrix"
      ],
      "metadata": {
        "id": "wuy0sMWFDZok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that when using Euclidean distance, documents that are not related still have a non-zero similarity, which is not ideal."
      ],
      "metadata": {
        "id": "_klt7UE7A07i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF"
      ],
      "metadata": {
        "id": "PKIDslEJtDfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's rerun the same experiment with TF-IDF"
      ],
      "metadata": {
        "id": "WCbsubEytFcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "NaaXmxO9tIur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_model =\n",
        "\n",
        "tfidf_model.\n",
        "\n",
        "df_tfidf_transformed = tfidf_model.\n",
        "tfidf_vectors =\n",
        "tfidf_vectors"
      ],
      "metadata": {
        "id": "5Y3Yrn35tWth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application: Spam Detection\n",
        "\n",
        "Let's apply what we have learned to a dataset of emails. Each email could be ham or scam.\n"
      ],
      "metadata": {
        "id": "W5TQ8S46DWsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/elhamod/BA820/main/Hands-on/04-text-mining/hamspam.csv\"\n",
        "df_sms = pd.read_csv(url, names = ['type', 'text'], index_col='type')\n",
        "\n",
        "X = df_sms['text']\n",
        "y = df_sms.index\n",
        "\n",
        "# df = pd.DataFrame(df.text.str.lower()) # We can try lower-casing.\n",
        "\n",
        "df_sms"
      ],
      "metadata": {
        "id": "GyRLnt9goWpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised step: Vectorize!**"
      ],
      "metadata": {
        "id": "QtY2NMe8BwhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xZYMJeP7DiEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer() #lowercase=False\n",
        "\n",
        "# create the vectorizer.\n",
        "X_train_counts = vectorizer.\n",
        "\n",
        "# vectorize the test set\n",
        "X_test_counts = vectorizer."
      ],
      "metadata": {
        "id": "796OX9q5rJRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_counts.toarray().shape"
      ],
      "metadata": {
        "id": "OAXfE1v1rKbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** How many unique tokens do we have? in this email dataset?"
      ],
      "metadata": {
        "id": "QMyCi-9sGQd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised learning: Let's train a classifier and look at the results.**"
      ],
      "metadata": {
        "id": "OQ1tWq-1Cdz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_counts, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test_counts)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred, normalize='true'), columns=model.classes_, index=model.classes_ )"
      ],
      "metadata": {
        "id": "7gVd26C8qN4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "- Would making tokens lower case help?\n",
        "- Explore whether using a purly supervised approach leads to better results (e.g., by feature engineering the text. Think text length, number of exclamation marks, etc.)\n",
        "- What tokens is logistic regression considering the most in its classification decision?"
      ],
      "metadata": {
        "id": "o950Gn0rBbrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's explore n-grams"
      ],
      "metadata": {
        "id": "Xg780VNTyAU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Some parameters we could play with.\n",
        "\n",
        "lowercase= True\n",
        "n_gram_range = (1,3)"
      ],
      "metadata": {
        "id": "vhICQmxg1gtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TxNZ2Twecazf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Let's vectorize."
      ],
      "metadata": {
        "id": "2ZhU9A9nz2Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def tokenize(doc):\n",
        "  tokens = word_tokenize(doc)\n",
        "\n",
        "  # Remove punctuation\n",
        "  tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "9E9N_ZIvM0GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "vectorizer_ngram = CountVectorizer(lowercase=lowercase, ngram_range=, tokenizer=tokenize, stop_words='english')"
      ],
      "metadata": {
        "id": "BvreDSAYyFG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit on the training data. Also transform it.\n",
        "X_train_ngram = vectorizer_ngram.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data.\n",
        "X_test_ngram = vectorizer_ngram.transform(X_test)\n",
        "\n",
        "X_train_ngram_df = pd.DataFrame(X_train_ngram.toarray(), columns=vectorizer_ngram.get_feature_names_out())"
      ],
      "metadata": {
        "id": "YlhSE-4_MtKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ngram_df"
      ],
      "metadata": {
        "id": "NEY_Isou1OxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many \"1\"s are there here? What does that mean?"
      ],
      "metadata": {
        "id": "rpeQPOeANTZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ngram_df.astype(int).sum().sum()"
      ],
      "metadata": {
        "id": "78wxSInX0qUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Predict!"
      ],
      "metadata": {
        "id": "YLQFRssosnEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model_ngram = LogisticRegression(max_iter=1000)\n",
        "model_ngram.fit(X_train_ngram, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_ngram = model_ngram.predict(X_test_ngram)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ngram = accuracy_score(y_test, y_pred_ngram)\n",
        "f1_score_ngram = sklearn.metrics.f1_score(y_test, y_pred_ngram, pos_label=\"spam\")\n",
        "print(f\"Accuracy: {accuracy_ngram}\")\n",
        "print(f\"f1_score: {f1_score_ngram}\")\n",
        "print(sklearn.metrics.classification_report(y_test,y_pred_ngram))\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred_ngram, normalize='true'), columns=model_ngram.classes_, index=model_ngram.classes_ )"
      ],
      "metadata": {
        "id": "zcrxYKixsnZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "- What would happen if I use a large context (i.e., large n)?\n",
        "- What would happen if I use a large range of \"n\"s (i.e., mixed n-gram model)\n",
        "- Would the results change if we balance the dataset?"
      ],
      "metadata": {
        "id": "hbnfm9jK0YNJ"
      }
    }
  ]
}